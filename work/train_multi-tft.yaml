model_path: multi-tft.ckpt
tune: false
dataset_kwargs:
  start: -86400
  end: ~
  split: 0.8
  engine_kwargs:
    btcusdt:
      orderbook:
        url: sqlite:///btcusdt@depth5@1000ms.db
      trades:
        url: sqlite:///btcusdt@trade.db  
      news:
        url: sqlite:///news.db
    ethusdt:
      orderbook:
        url: sqlite:///ethusdt@depth5@1000ms.db
      trades:
        url: sqlite:///ethusdt@trade.db  
      news:
        url: sqlite:///news.db
    ethbtc:
      orderbook:
        url: sqlite:///ethbtc@depth5@1000ms.db
      trades:
        url: sqlite:///ethbtc@trade.db  
      news:
        url: sqlite:///news.db
  read_kwargs:
    btcusdt:
      orderbook:
        name: data
      trades:
        name: data
      news:
        name: data
    ethusdt:
      orderbook:
        name: data
      trades:
        name: data
      news:
        name: data
    ethbtc:
      orderbook:
        name: data
      trades:
        name: data
      news:
        name: data
  features_kwargs:
    post_features:
    - time_idx
    - group
    pre_features:
    - m_p
    resample_kwargs:
      rule: 1S
    aggregate_kwargs:
      orderbook:
        func: last
      trades:
        func: last
      news:
        func: last
    interpolate_kwargs:
      orderbook:
        method: pad
      trades:
        method: pad
      news: ~
  dataset_kwargs:
    time_idx: time_idx
    allow_missing_timesteps: true
    add_relative_time_idx: true
    target: m_p
    group_ids:
    - group
    max_encoder_length: 10
    max_prediction_length: 5
    time_varying_unknown_reals:
    - m_p
    - a_p_0
    - b_p_0
    target_normalizer:
      class: GroupNormalizer
      groups: [ group ]
    scalers:
      class: GroupNormalizer
      groups: [ group ]
dataloader_kwargs:
  train:
    train: true
    num_workers: 2
    batch_size: 32
  val:
    train: false
    num_workers: 2
    batch_size: 32
model_kwargs:
  class: TemporalFusionTransformer
  learning_rate: 0.03
  hidden_size: 32
  attention_head_size: 1
  dropout: 0.1
  hidden_continuous_size: 16
  output_size: 7
  loss:
    class: QuantileLoss
  log_interval: 0
  reduce_on_plateau_patience: 2
trainer_kwargs:
  max_epochs: 1
  accelerator: cpu  # gpu
  # devices: [ 0 ]
  gradient_clip_val: 0.1
  log_every_n_steps: 50
  # limit_train_batches: 10
  logger:
  - class: TensorBoardLogger
    save_dir: tensorboard
    name: tft
  callbacks:
  - class: LearningRateMonitor
    logging_interval: step
  - class: ModelCheckpoint
    monitor: val_loss
    filename: '{epoch}-{step}-{val_loss:.3f}'
    save_last: true
    save_top_k: 1
  - class: EarlyStopping
    monitor: val_loss
    min_delta: 0
    patience: 4
    verbose: false
    mode: min
