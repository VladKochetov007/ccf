n_envs: 32
seed: 0
verbose: 0
do_train: true
do_backtest: true
split: ~
model_name: ppo-fees_1em4-last_1h
is_tune: true
parent_name: ppo-fees_1em4-last_1h
parent_version: ~  # last
learn_kwargs:
  # tb_log_name: ppo-fees_1em5
  total_timesteps: 1024000  # number of rollouts (epochs) = total_timesteps / (n_steps * n_envs)
  reset_num_timesteps: false
  callback: ~ 
  log_interval: 1  #  The number of rollouts before logging
  progress_bar: true
env_kwargs:
 cash: 30.0
 commission: 1e-4
 window_size: 20
data_kwargs:
  # test_size: 10000
  reload: true
  # start: '2023-04-03T00:00:00+00:00'
  # stop: '2023-04-04T00:00:00+00:00'
  start: -3600e9
  stop: ~
  model_name: 'influxdb-mid-rat-ema_vwap_base_10_1_1-mel_20-mpl_20-no_cat-no_group'
  model_version: 4
  exchange: binance
  base: btc
  quote: usdt
  horizon: 20
  unit_scale: 0.001
model_kwargs:
  class: PPO
  policy: MlpPolicy
  learning_rate: 0.0003
  n_steps: 1024  # The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)
  batch_size: 32
  n_epochs: 10  # Number of epoch when optimizing the surrogate loss
  gamma: 0.99 
  gae_lambda: 0.95
  clip_range: 0.2
  seed: 0
  device: cuda
  clip_range_vf: ~ 
  normalize_advantage: True
  ent_coef: 0.0 
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False 
  sde_sample_freq: -1 
  target_kl: ~
  # tensorboard_log: "./tensorboard/ppo"
  policy_kwargs: ~
    # net_arch: {pi: [32, 32], vf: [64, 64]}
    # net_arch: [128, 128]
