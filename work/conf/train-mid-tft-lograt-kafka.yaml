defaults:
 - _self_
 - partitioner@create_dataset_kwargs.agents.a_1.consumer.partitioners.feature: feature
 - dataset@create_dataset_kwargs.dataset_kwargs: mid-lograt-kafka

model_path: mid-tft-lograt-kafka.ckpt
tune: false
create_dataset_kwargs:
  verbose: false
  quant: 1.0e+9
  size: 50
  replace_nan: 0.0
  agents:
    a_1: 
      class: KafkaDataset
      verbose: true
      topics: [ feature ]
      feature_keys:
        Delta-default-lograt: [ binance/btc/usdt, binance/eth/usdt, binance/eth/btc ]
      consumer:
        bootstrap_servers: "kafka:9092"
  split: 0.8
dataloader_kwargs:
  train:
    train: true
    num_workers: 0
    batch_size: 8
  val:
    train: false
    num_workers: 0
    batch_size: 8
model_kwargs:
  class: TemporalFusionTransformer
  learning_rate: 0.0078125
  hidden_size: 8
  attention_head_size: 1
  dropout: 0
  hidden_continuous_size: 8
  lstm_layers: 1
  # output_size: 7  # Inferred from loss
  loss:
    class: QuantileLoss
  log_interval: 0
  reduce_on_plateau_patience: 2
  reduce_on_plateau_reduction: 2
  reduce_on_plateau_min_lr: 1.0e-10
  weight_decay: 0
trainer_kwargs:
  max_epochs: 2
  accelerator: cpu  # gpu
  devices: ~
  gradient_clip_val: 0
  log_every_n_steps: 100
  limit_train_batches: ~
  logger:
  - class: TensorBoardLogger
    save_dir: tensorboard
    name: mid_tft_lograt_min
  callbacks:
  - class: LearningRateMonitor
    logging_interval: step
  - class: ModelCheckpoint
    monitor: val_loss
    filename: '{epoch}-{step}-{val_loss:.3f}'
    save_last: true
    save_top_k: 1
  - class: EarlyStopping
    monitor: val_loss
    min_delta: 0
    patience: 4
    verbose: false
    mode: min
