n_envs: 1
seed: ~
verbose: 0
do_train: true
do_backtest: false
split: ~
model_name: ppo-binance-btc-tusd-last_30m-tune
is_tune: true
parent_name: ppo-binance-btc-tusd-last_30m-tune
parent_version: ~  # last
learn_kwargs:
  # tb_log_name: ppo-fees_1em5
  total_timesteps: 150000  # number of rollouts (epochs) = total_timesteps / (n_steps * n_envs)
  reset_num_timesteps: false
  callback: ~
  log_interval: 1  #  The number of rollouts before logging
  progress_bar: true
env_kwargs:
 cash: 100.0
 commission: 0.0
 window_size: 20
data_kwargs:
  # test_size: 10000
  reload: true
  do_dump: false
  # start: '2023-04-03T00:00:00+00:00'
  # stop: '2023-04-04T00:00:00+00:00'
  start: -1800e9
  stop: ~
  model_name: 'influxdb-mid-rat-ema_vwap_base_10_1_1-mel_20-mpl_20-no_cat-no_group'
  model_version: 4
  exchange: binance
  base: btc
  quote: tusd
  horizon: 20
  unit_scale: 0.001
model_kwargs:
  class: PPO
  policy: MlpPolicy
  learning_rate: 0.0003
  n_steps: 2048  # The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel) NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)
  batch_size: 64
  n_epochs: 10  # Number of epoch when optimizing the surrogate loss
  gamma: 0.99 
  gae_lambda: 0.95
  clip_range: 0.2
  seed: ~
  device: cuda
  clip_range_vf: ~ 
  normalize_advantage: true
  ent_coef: 0.0 
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1 
  target_kl: ~
  # tensorboard_log: "./tensorboard/ppo"
  policy_kwargs: ~
    # net_arch: {pi: [32, 32], vf: [64, 64]}
    # net_arch: [128, 128]
